{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2 align=\"center\">DLOps - Exportr um modelo a ONNX</h2>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Data Scientist.: PhD.Eddy Giusepe Chirinos Isidro"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ONNX"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Depois de treinar vários modelos, compará-los e decidir qual usaremos em produção, temos que exportá-lo. Para isso, existem alternativas, em função da aplicação (desde desplegar um modelo em dispositivos celulares ou IoT até em servidores na nuvem accesíveis através de uma API). Aqui consideramos que nosso modelo será executado num servidor em nuvem, o qual é o mais comum já que desta maneira podemos controlar os recursos computacionais disponíveis para sua execução, monitorar, desplegar novas versões facilmente, etc. Em nosso caso como treinamos os modelos usando Pytorch e Pytorch Lightning, poderíamos usar qualquer framework em Python que nos permita server as predições através de internet, como por exemplo: [Flask](https://flask.palletsprojects.com/en/2.0.x/) ou [FastAPI](https://fastapi.tiangolo.com/). O principal problema desta opção é que teremos que carregar todas as bibliotecas (e suas dependências) em nossa API, o qual resultará em uma carga muito pesada. Recentemente, Pytorch inclui uma solução dedicada para este caso de uso, [Torchserve](https://pytorch.org/serve/) que se bem nos oferece uma solução otimizada para server modelos em produção, está limitada ao uso de modelos Pytorch.\n",
    "\n",
    "\n",
    "É neste ponto em que entra [ONNX](https://onnx.ai/), um standard aberto para a representação de Redes Neurais que permite a interoperabilidade  entre bibliotecas e oferece uma solução otimizada para server modelos em produção (tanto na nuvem como em dispositivos celulares). Desta maneira podemos desacoplar o treinamento de modelos de sua posta em produção, utilizando em cada caso as ferramentas preferidas para seu desenvolvimento. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Exportar um modelo a ONNX"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Vamos a ver como podemos exportar um modelo treinado a ONNX. Em primeiro lugar, carregamos o `checkpoint` desejado (o qual foi gerado no script anterior)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Sequential(\n",
       "  (0): Linear(in_features=784, out_features=100, bias=True)\n",
       "  (1): ReLU()\n",
       "  (2): Linear(in_features=100, out_features=1, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from src import *\n",
    "\n",
    "module = MNISTModule.load_from_checkpoint('/home/eddygiusepe/17_Pytorch/DLOps_MLOps_para_Deep_Learning/final.ckpt')\n",
    "module.mlp"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "É Uma boa prática evaliar nosso modelo antes e depois de exportá-lo para ter certeza de que todo funciona corretamente."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.8999999761581421"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch \n",
    "\n",
    "dm = MNISTDataModule(**module.hparams['datamodule'])\n",
    "dm.setup()\n",
    "\n",
    "def torch_eval():\n",
    "    module.eval()\n",
    "    with torch.no_grad():\n",
    "        preds, labels = torch.tensor([]), torch.tensor([])\n",
    "        for imgs, _labels in dm.val_dataloader():\n",
    "            outputs = module.predict(imgs) > 0.5\n",
    "            preds = torch.cat([preds, outputs.cpu().long()])\n",
    "            labels = torch.cat([labels, _labels])\n",
    "\n",
    "    acc = (preds == labels).float().mean()\n",
    "    return acc.item()\n",
    "\n",
    "torch_eval()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Pytorch Lightning nos permite exportar um modelo a `ONNX` de maneira muito simples com a seguinte linha:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_sample = torch.randint(0, 255, (1, 28, 28), dtype=torch.uint8)\n",
    "module.to_onnx(\n",
    "    'models/binary_classifier_3.onnx', # Nome do modelo a salvar\n",
    "    input_sample, # exemplo de entrada\n",
    "    export_params=True, # Exportar os parâmetros do modelo\n",
    "    opset_version=11, # En função das ops no modelo, se pode trocar o opset\n",
    "    input_names = ['input'], # Nome da entrada\tpara usar em produção\n",
    "    output_names = ['output'],  # Nome da saída para usar em produção\n",
    "    dynamic_axes={  # Para poder ter diferentes batch sizes\n",
    "        'input' : {0 : 'batch_size'},\n",
    "        'output' : {0 : 'batch_size'},\n",
    "    },\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Como observas temos que indicar o nome do modelo e a pasta onde queremos salvar, dar umas entradas de exemplo (que ONNX usará para identificar todas as operações que se realizará dentro do modelo e salvá-las), se queremos exportar os parâmetros do modelo, a versão do `opset` (este é o conjunto de operações suportadas, que vâ mudando a medida que se adicionam novas funcionalidades) e, opcionalmente, nomes para as entradas e saídas (isto é importante se nosso modelo tem várias entradas e/ou saídas) assim como indicar que eixos dinâmicos (útil para poder usar o modelo em produção com diferentes *bacth sizes*)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ONNXRuntime"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Una vez hemos exportado nuestro modelo podemos cargarlo y ejecutarlo usando el *ONNXRuntime*. Esta es una de las ventajas de ONNX, y es que existen *runtimes* para multiples entornos y lenguajes. Así pues, podrás entrenar el modelo en `Python` y luego desplegarlo tanto en `Python` como en la web con `Javascript`, en dipositivos moviles con `Android` o `iOS`, etc.\n",
    "\n",
    "> En Python, puedes instalarlo con el comando `pip install onnxruntime`.\n",
    "\n",
    "Para cargar el modelo instanciamos una `InferenceSession` con el `path` al modelo exportado. Luego, definiermos las entradas al modelo usando el un `dict` con el nombre definido en la fase de exportación. Date cuenta que ahora el modelo usará como entradas array de `NumPy`, ya que en este entorno `Pytorch` ya no existe. Si que es importante, sin embargo, que uses el mismo tamaño y tipo de datos que usaste en el entrenamiento. Por último, podemos obtener las salidas del modelo usando el método `run()`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(10,)"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import onnxruntime as ort \n",
    "import numpy as np\n",
    "\n",
    "# Carregar nosso Modelo\n",
    "ort_session = ort.InferenceSession('./models/binary_classifier_3.onnx')\n",
    "\n",
    "ort_inputs = {\n",
    "    \"input\": np.random.randint(0, 255, (10, 28, 28), dtype=np.uint8),\n",
    "}\n",
    "\n",
    "\n",
    "# Para executá-lo\n",
    "ort_output = ort_session.run(['output'], ort_inputs)\n",
    "ort_output[0].shape\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "E, como comentava antes, é importante evaliar o modelo para ter certeza que foi exportado bem. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.9"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def onnx_eval():\n",
    "    with torch.no_grad():\n",
    "        preds, labels = [], torch.tensor([])\n",
    "        for imgs, _labels in dm.val_dataloader():\n",
    "            ort_inputs = {\n",
    "                \"input\": imgs.numpy(),\n",
    "            }\n",
    "            ort_output = ort_session.run(['output'], ort_inputs)[0]\n",
    "            outputs = ort_output > 0.5\n",
    "            preds += outputs.astype(int).tolist()\n",
    "            labels = torch.cat([labels, _labels])\n",
    "    acc = (np.array(preds) == labels.numpy()).astype(float).mean()\n",
    "    return acc \n",
    "\n",
    "onnx_eval()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "En este caso obtenemos la misma métrica de evaluación que la obtenida con el checkpoint, así que podemos estar tranquilos de que el modelo se comportará bien en producción."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Versionando Modelos"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Una vez hemos exportado nuestro modelo y hemos verificado que funciona bien podemos versionarlo de la misma manera que hicimos con nuestro dataset.\n",
    "\n",
    "Para ello, primero añadimos la carpeta `models` a nuestro repositorio.\n",
    "\n",
    "```\n",
    "dvc add models\n",
    "```\n",
    "\n",
    "De momento solo tenemos un modelo, el que acabamos de exportar, y podemos sincornizarlo con el repositorio remoto (en el que ya viven varias versiones de nuestro dataset) de la siguiente manera\n",
    "\n",
    "```\n",
    "dvc push\n",
    "```\n",
    "\n",
    "De esta manera, cualquier persona con accesso al proyecto en el que estamos trabajando podrá recuperar este modelo con el comando\n",
    "\n",
    "```\n",
    "dvc pull models.dvc\n",
    "```\n",
    "\n",
    "Puedes porbar que funcione eliminando la carpeta `models` y recuperándola con el comando anterior. Recuerda generar un nuevo tag y sincronizar con el repositorio de git.\n",
    "\n",
    "```\n",
    "git add .\n",
    "git commit -m \"primer modelo\"\n",
    "git push\n",
    "git tag -a v3 -m \"version 3\"\n",
    "git push origin --tags\n",
    "```\n",
    "\n",
    "A partir de ahora, al entrenar nuevos modelos, podemos añadrilos al repositorio con nuevo tag. Usar un modelo diferente en producción será tan sencillo como cambiar al tag adecuado, lo cual veremos más adelante."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.8.10 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "916dbcbb3f70747c44a77c7bcd40155683ae19c65e1c03b4aa3499c5328201f1"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
